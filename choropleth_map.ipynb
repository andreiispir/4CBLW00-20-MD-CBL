{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055459d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da9ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1\n",
      "Processed chunk 2\n",
      "Processed chunk 3\n",
      "Processed chunk 4\n",
      "Processed chunk 5\n",
      "Processed chunk 6\n",
      "Processed chunk 7\n",
      "Processed chunk 8\n",
      "Processed chunk 9\n",
      "Processed chunk 10\n",
      "Processed chunk 11\n",
      "Processed chunk 12\n",
      "Processed chunk 13\n",
      "Processed chunk 14\n",
      "Processed chunk 15\n",
      "Processed chunk 16\n",
      "Processed chunk 17\n",
      "Processed chunk 18\n",
      "Processed chunk 19\n",
      "Processed chunk 20\n",
      "Processed chunk 21\n",
      "Processed chunk 22\n",
      "Processed chunk 23\n",
      "Processed chunk 24\n",
      "Processed chunk 25\n",
      "Processed chunk 26\n",
      "Processed chunk 27\n",
      "Processed chunk 28\n",
      "Processed chunk 29\n",
      "Processed chunk 30\n",
      "Processed chunk 31\n",
      "Processed chunk 32\n",
      "Processed chunk 33\n",
      "Processed chunk 34\n",
      "Processed chunk 35\n",
      "Processed chunk 36\n",
      "Processed chunk 37\n",
      "Processed chunk 38\n",
      "Processed chunk 39\n",
      "Processed chunk 40\n",
      "Processed chunk 41\n",
      "Processed chunk 42\n",
      "Processed chunk 43\n",
      "Processed chunk 44\n",
      "Processed chunk 45\n",
      "Processed chunk 46\n",
      "Processed chunk 47\n",
      "Processed chunk 48\n",
      "Processed chunk 49\n",
      "Processed chunk 50\n",
      "Processed chunk 51\n",
      "Processed chunk 52\n",
      "Processed chunk 53\n",
      "Processed chunk 54\n",
      "Processed chunk 55\n",
      "Processed chunk 56\n",
      "Processed chunk 57\n",
      "Processed chunk 58\n",
      "Processed chunk 59\n",
      "Processed chunk 60\n",
      "Processed chunk 61\n",
      "Processed chunk 62\n",
      "Processed chunk 63\n",
      "Processed chunk 64\n",
      "Processed chunk 65\n",
      "Processed chunk 66\n",
      "Processed chunk 67\n",
      "Processed chunk 68\n",
      "Processed chunk 69\n",
      "Processed chunk 70\n",
      "Processed chunk 71\n",
      "Processed chunk 72\n",
      "Processed chunk 73\n",
      "Processed chunk 74\n",
      "Processed chunk 75\n",
      "Processed chunk 76\n",
      "Processed chunk 77\n",
      "Processed chunk 78\n",
      "Processed chunk 79\n",
      "Processed chunk 80\n",
      "Processed chunk 81\n"
     ]
    }
   ],
   "source": [
    " # --- Configuration ---\n",
    "csv_filename = 'data.csv'\n",
    "geojson_path = r'C:\\Users\\20232645\\Desktop\\TUe\\Y2\\Q4\\Data Challenge 2\\4CBLW00-20-MD-CBL\\LSOA boundries of feb 2025\\combined_lsoa.geojson'\n",
    "output_dir = 'choropleth_maps'\n",
    "chunk_size = 1_500_000\n",
    "\n",
    "gdf_lsoas = gpd.read_file(geojson_path)\n",
    "london_lsoa_codes = set(gdf_lsoas['Description']) \n",
    "\n",
    "# London's bounding box\n",
    "lat_min, lat_max = 51.2867602, 51.6918741\n",
    "lon_min, lon_max = -0.5103751, 0.3340155\n",
    "\n",
    "columns_to_keep = ['Month', 'Latitude', 'Longitude', 'Location', 'LSOA code', 'Crime type']\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "# --- Handle bad lines ---\n",
    "bad_line_counter = 0\n",
    "def handle_bad_line(bad_line):\n",
    "    global bad_line_counter\n",
    "    bad_line_counter += 1\n",
    "    return None\n",
    "\n",
    "# --- Load and preprocess data in chunks ---\n",
    "csv_path = os.path.join(os.getcwd(), csv_filename)\n",
    "chunks = pd.read_csv(csv_path,\n",
    "                     usecols = columns_to_keep,\n",
    "                     chunksize = chunk_size,\n",
    "                     engine = 'python',\n",
    "                     on_bad_lines = handle_bad_line)\n",
    "\n",
    "dfs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk = chunk.dropna(subset = ['Latitude', 'Longitude', 'LSOA code'])\n",
    "    chunk = chunk[\n",
    "    (chunk['Latitude'] >= lat_min) & (chunk['Latitude'] <= lat_max) &\n",
    "    (chunk['Longitude'] >= lon_min) & (chunk['Longitude'] <= lon_max)\n",
    "]\n",
    "    print(f\"Processed chunk {i+1}\")\n",
    "    dfs.append(chunk)\n",
    "\n",
    "df_london = pd.concat(dfs, ignore_index = True)\n",
    "df_london['Month'] = pd.to_datetime(df_london['Month'], errors = 'coerce')\n",
    "print(f\"Total valid rows: {len(df_london)} | Skipped bad rows: {bad_line_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a40ce098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All maps generated.\n"
     ]
    }
   ],
   "source": [
    "# --- Load LSOA GeoJSON ---\n",
    "lsoa_geo = gpd.read_file(r'C:\\Users\\20232645\\Desktop\\TUe\\Y2\\Q4\\Data Challenge 2\\4CBLW00-20-MD-CBL\\LSOA boundries of feb 2025\\combined_lsoa.geojson')\n",
    "# print(lsoa_geo.columns)\n",
    "# print(lsoa_geo.head())\n",
    "\n",
    "# --- Generate choropleth maps by year ---\n",
    "years = df_london['Month'].dt.year.dropna().unique()\n",
    "\n",
    "for year in sorted(years):\n",
    "    print(f\"Generating map for {year}...\")\n",
    "    df_year = df_london[df_london['Month'].dt.year == year]\n",
    "    lsoa_counts = df_year.groupby('LSOA code').size().reset_index(name='crime_count')\n",
    "\n",
    "    merged = lsoa_geo.merge(lsoa_counts, left_on='Description', right_on='LSOA code', how='left')\n",
    "    merged['crime_count'] = merged['crime_count'].fillna(0)\n",
    "\n",
    "    # Create folium map\n",
    "    m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "    folium.Choropleth(\n",
    "        geo_data=merged,\n",
    "        name='choropleth',\n",
    "        data=merged,\n",
    "        columns=['Description', 'crime_count'],\n",
    "        key_on='feature.properties.Description',\n",
    "        fill_color='YlOrRd',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        legend_name=f'Crime Count per LSOA ({year})'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # # Optional: Add popups\n",
    "    # folium.GeoJsonTooltip(fields=['LSOA11CD', 'crime_count'],\n",
    "    #                       aliases=[\"LSOA Code:\", \"Crimes:\"],\n",
    "    #                       sticky=False).add_to(folium.GeoJson(merged).add_to(m))\n",
    "\n",
    "    m.save(os.path.join(output_dir, f'crime_density_{year}.html'))\n",
    "\n",
    "print(\"All maps generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c38509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
